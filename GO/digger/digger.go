package digger

import (
	"container/list"
	"errors"
	"fmt"
	"math/rand"
	"net/http"
	"net/http/cookiejar"
	"net/url"
	"os"
	"regexp"
	"strings"
	"sync"
	"time"

	"github.com/astaxie/beego/logs"
)

//ä¸€äº›å…¬ç”¨é…ç½®ã€å‚æ•°
var (
	totalSizeLimit int    //å›¾ç‰‡æ€»å¤§å°é™åˆ¶,MB
	minSizeLimit   int    //æ–‡ä»¶å¤§å°æœ€å°å€¼ï¼ŒKB
	maxSizeLimit   int    //æ–‡ä»¶å¤§å°æœ€å¤§å€¼ï¼ŒKB
	numberLimit    int    //å›¾ç‰‡ä¸‹è½½æ•°é‡é™åˆ¶
	threadLimit    int    //ä¸‹è½½å¼•æ“æ•°é‡é™åˆ¶
	waitTimeLimit  int    //æœ€é•¿ç­‰å¾…æ—¶é—´ï¼Œå•ä½ç§’
	intervalTime   int    //ç­‰å¾…é—´éš”ï¼Œç§’
	savePath       string //ä¿å­˜è·¯å¾„
	method         string //ç­–ç•¥ï¼šBFS\DFS\FOR\LIST
	baseUrl        string //BFS\DFSè½¦è£‚ä¸‹çš„ç½‘é¡µå…¥å£é“¾æ¥ï¼Œæˆ–FORç­–ç•¥ä¸‹çš„æ¨¡æ¿, listç­–ç•¥ä¸‹æ”¾urlåˆ—è¡¨
	linkKey        string //è½¬è·³é“¾æ¥å…³é”®å­—
	targetKey      string //åŒ…å«ç›®æ ‡å›¾ç‰‡é“¾æ¥çš„è¡¨æƒ…å…³é”®å­—
	startPoint     int    //FORç­–ç•¥ä¸‹çš„å˜é‡èµ·ç‚¹(åŒ…å«)
	endPoint       int    //FORç­–ç•¥ä¸‹çš„å˜è„¸ç»ˆç‚¹(åŒ…å«)
)

//ä¸€äº›ç»Ÿè®¡æ•°å€¼
var (
	totalBytes  int       //å·²ä¸‹è½½å›¾ç‰‡çš„æ€»å¤§å°
	totalNumber int       //å·²ä¸‹è½½å›¾ç‰‡çš„ä¸­æ•°é‡
	pageNumber  int       //å·²ç»è®¿é—®çš„é¡µé¢æ•°é‡
	tmpBytes    int       //å•ä½æ—¶é—´å†…ä¸‹è½½å›¾ç‰‡çš„æ€»å¤§å°
	totalTime   int       //æ€»è¿è¡Œæ—¶é—´ï¼Œç§’
	lastTime    time.Time //ä¸Šæ¬¡ç»Ÿè®¡ä¸‹è½½é€Ÿåº¦çš„æ—¶é—´
	startTime   time.Time //ä¸Šæ¬¡ç‚¹å‡»å¼€å§‹æˆ–ç»§ç»­çš„æ—¶é—´
)

//ä¸€äº›å…¨å±€æ•°æ®å®¹å™¨
var (
	foundPageList *list.List      //å·²ç»å‘ç°çš„ç½‘é¡µé“¾æ¥åœ°å€åˆ—è¡¨
	lastPageEle   *list.Element   //ä¼˜å…ˆå¾…å¤„ç†çš„ç½‘é¡µåœ°å€
	foundImgList  *list.List      //å·²ç»å‘ç°çš„å›¾ç‰‡é“¾æ¥åˆ—è¡¨
	lastImgEle    *list.Element   //ä¼˜å…ˆå¾…å¤„ç†çš„å›¾ç‰‡é“¾æ¥
	pageUrlMap    map[string]bool //è®°å½•å·²ç»è®¿é—®è¿‡çš„é¡µé¢é¿å…é‡å¤çˆ¬å–
	imgUrlMap     map[string]bool //è®°å½•å·²ç»ä¸‹è½½è¿‡çš„å›¾ç‰‡é¿å…é‡å¤ä¸‹è½½
)

//ä¸€äº›å…¨å±€å˜é‡æˆ–å¯¹è±¡
var (
	diggerState     int32        //å·¥ä½œçŠ¶æ€ï¼š0æœªå¼€å§‹æˆ–å·²ç»ˆæ­¢ï¼Œ1è¿è¡Œä¸­ï¼Œ2æš‚åœä¸­
	randMachine     *rand.Rand   //ç”¨æˆ·åˆ›å»ºéšæœºæ•°çš„å¯¹è±¡
	mainClient      *http.Client //ç”¨äºå‘é€httpè¯·æ±‚çš„å®¢æˆ·ç«¯å¯¹è±¡
	getNameMutex    *sync.Mutex  //åŒæ­¥é”ï¼Œç”Ÿæˆéšæœºæ•°æ—¶ç”¨
	updataSizeMutex *sync.Mutex  //åŒæ­¥é”ï¼Œæ›´æ–°å·²ä¸‹è½½å›¾ç‰‡å¤§å°æ—¶ç”¨
)

//ä¸€äº›å…¨å±€æ­£åˆ™è¡¨è¾¾å¼å¯¹è±¡
var (
	regexpFindAllATag   *regexp.Regexp //æ‰¾å‡ºæ‰€æœ‰çš„ <a> æ ‡ç­¾
	regexpFindAllImgTag *regexp.Regexp //æ‰¾å‡ºæ‰€æœ‰çš„ <img> æ ‡ç­¾
)

func init() {
	diggerState = 0 //æœªå¼€å§‹
	randMachine = rand.New(rand.NewSource(time.Now().UnixNano()))
	mainClient = new(http.Client)
	foundPageList = list.New()
	foundImgList = list.New()
	pageUrlMap = make(map[string]bool)
	imgUrlMap = make(map[string]bool)
	if tmpJar, err := cookiejar.New(nil); err != nil {
		logs.Warn("Create a cookieJar fail: err=%v", err)
		tmpJar = nil
	} else {
		mainClient.Jar = tmpJar
	}
	//æœªç»æµ‹è¯•ä¿®æ”¹ä»¥ä¸‹æ­£åˆ™å¯èƒ½å¼•å‘panic
	regexpFindAllATag = regexp.MustCompile(`<a [^>]*href=[^>]*>`)
	regexpFindAllImgTag = regexp.MustCompile(`<img [^>]*src=[^>]*>`)
}

//å¼€å§‹å·¥ä½œï¼Œconfig ä¸ºæŒ‡å®šå·¥ä½œæ–¹å¼çš„é…ç½®è¯´æ˜
func StartDigger(config string) error {
	var err error
	if err := setUpConfig(config); err != nil { //åˆå§‹åŒ–
		logs.Error("Setupconfig fail: %v", err)
		return err
	}
	if suc, err := canVisitBaseUrl(); !suc { //æ£€æŸ¥ç½‘ç»œçŠ¶æ€
		logs.Warn("Can't not visit BaseUrl, err=%v", err)
		return err
	}
	err = ContinueDigger() //å¼€å§‹å·¥ä½œ
	if err != nil {
		logs.Error("Start or continue fail: %v", err)
		return err
	}
	return nil
}

//æš‚åœå·¥ä½œ,ä¿ç•™çŠ¶æ€ ğŸ¢
func PauseDigger() error {
	return nil
}

//å¼€å§‹æˆ–ç»§ç»­å·¥ä½œ
func ContinueDigger() error {
	var err error
	switch diggerState {
	case 1: //æ­£åœ¨å·¥ä½œä¸­
		err = errors.New("Can not start or continue because digger is running")
	case 0, 2:
		if method == "BFS" {
			err = runBFS()
		} else if method == "DFS" {
			err = runDFS()
		} else if method == "FOR" {
			err = runFOR()
		} else if method == "LIST" {
			err = runLIST()
		} else {
			err = errors.New("Unexpect method: mthod=" + method)
		}
	default:
		err = fmt.Errorf("Unknow diggerState: diggerState=%d", diggerState)
	}
	if err != nil {
		logs.Error(err)
		return err
	}
	return nil
}

//ç»ˆæ­¢å·¥ä½œæ¸…æ¥šçŠ¶æ€ ğŸ¢
func StopDigger() error {
	return nil
}

//============================= åŠŸèƒ½æµ‹è¯• =============================
func TEST1() {
	var link []string
	if err := getAllSpeciicPageLink("https://tb1.bdstatic.com/", &TmphtmlCode, &link); err != nil {
		logs.Error(err)
		return
	}
	fmt.Println(len(link))
	for i := 0; i < len(link); i++ {
		fmt.Println(link[i])
	}
	os.Exit(0)
}

//============================= ç§æœ‰å‡½æ•°/å·¥å…·å‡½æ•° =====================

//å¼€å§‹æˆ–ç»§ç»­BFSç­–ç•¥å·¥ä½œ
func runBFS() error {
	var err error
	pageHtml, url := "", "" //pageHtmlæš‚å­˜é¡µé¢çš„htmlä»£ç 
	if url = lastPageEle.Value.(string); url == "" {
		err = errors.New("lastPageEle is empty string")
		logs.Error(err)
		return err
	}
	lastPageEle = lastPageEle.Next()
	if err = getHtmlCodeOfUrl(url, &pageHtml); err != nil {
		logs.Warn("getHtmlCodeOfUrl fail: url=%s  err=%v", url, err)
		return err
	}
	//è·å–ç¬¦åˆè½¬è·³æ¡ä»¶çš„é“¾æ¥
	var pagelink []string
	if err = getAllSpeciicPageLink(url, &pageHtml, &pagelink); err != nil {
		logs.Error("Get specific pagelink from pageHtml fail: err=%v", err)
		return err
	}
	if len(pagelink) == 0 {
		logs.Warn("No pagelink found in pageHtml, url=%v", url)
	}
	//è·å–æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„å›¾ç‰‡é“¾æ¥
	var imgLink []string
	if err = getAllSpeciicImgLink(url, &pageHtml, &imgLink); err != nil {
		logs.Error("Get specific images link from pageHtml fail: err=%v", err)
		return err
	}
	if len(imgLink) == 0 {
		logs.Warn("No images link found in pageHtml, url=%v", url)
	}

	return nil
}

//å¼€å§‹æˆ–ç»§ç»­DFSç­–ç•¥å·¥ä½œ  ğŸ¢
func runDFS() error {
	logs.Warn("TODO: runDFS()")
	return nil
}

//å¼€å§‹æˆ–ç»§ç»­FORç­–ç•¥å·¥ä½œ  ğŸ¢
func runFOR() error {
	logs.Warn("TODO: runFOR()")
	return nil
}

//å¼€å§‹æˆ–ç»§ç»­LISTç­–ç•¥å·¥ä½œ ğŸ¢
func runLIST() error {
	logs.Warn("TODO: runLIST()")
	return nil
}

//è·å¾—htmlCodeä¸­å…¨éƒ¨ç¬¦åˆé…ç½®æŒ‡å®šçš„è½¬è·³é“¾æ¥ï¼Œå†™åˆ°link[]ä¸­
//baseUrlä¸ºè·å¾—é¡µé¢çš„URLï¼Œç”¨äºå°†å¾—åˆ°çš„ç›¸å¯¹é“¾æ¥è½¬æ¢æˆç»å¯¹é“¾æ¥
func getAllSpeciicPageLink(baseUrl string, htmlCode *string, link *[]string) error {
	if htmlCode == nil || *htmlCode == "" {
		return errors.New("htmlCode is empty")
	}
	if *link == nil {
		*link = make([]string, 0)
	} else if len(*link) != 0 {
		return errors.New("link[] not empty")
	}
	//å…ˆè·å–åŒ…å«é“¾æ¥çš„<a>æ ‡ç­¾
	allATag := regexpFindAllATag.FindAllString(*htmlCode, -1)
	if len(allATag) == 0 {
		logs.Warn("find zero <a> tag from htmlCode")
		return nil
	}
	//ä»<a>æ ‡ç­¾ä¸­ç­›é€‰å‡ºé“¾æ¥ï¼Œè‹¥é…ç½®æœ‰æŒ‡å®šå…³é”®å­—åˆ™åªä»åŒ…å«å…³é”®å­—çš„æ ‡ç­¾ä¸­å–
	regexpFindLink := regexp.MustCompile(`href="[^"]*`)
	for i := 0; i < len(allATag); i++ {
		if linkKey != "" && !strings.Contains(allATag[i], linkKey) {
			continue
		}
		tmpLink := regexpFindLink.FindString(allATag[i])
		if len(tmpLink) < 7 {
			logs.Warn("find a danger url: %s", tmpLink)
			continue
		}
		tmpLink = tmpLink[6:] //å»é™¤href="å‰ç¼€
		//å°†ç›¸å¯¹é“¾æ¥è½¬æ¢æˆç»å¯¹é“¾æ¥
		if err := CheckUrlAndConver(baseUrl, &tmpLink); err != nil {
			logs.Warn("check url %s not pass, err=%v", tmpLink, err)
			continue
		}
		*link = append(*link, tmpLink)
	}
	return nil
}

//è·å¾—htmlCodeä¸­å…¨éƒ¨ç¬¦åˆé…ç½®æŒ‡å®šçš„å›¾ç‰‡é“¾æ¥ï¼Œå†™åˆ°link[]ä¸­
//baseUrlä¸ºè·å¾—é¡µé¢çš„URLï¼Œç”¨äºå°†å¾—åˆ°çš„ç›¸å¯¹é“¾æ¥è½¬æ¢æˆç»å¯¹é“¾æ¥
func getAllSpeciicImgLink(baseUrl string, htmlCode *string, link *[]string) error {
	if htmlCode == nil || *htmlCode == "" {
		return errors.New("htmlCode is empty")
	}
	if *link == nil {
		*link = make([]string, 0)
	} else if len(*link) != 0 {
		return errors.New("link[] not empty")
	}
	//å…ˆè·å–åŒ…å«é“¾æ¥çš„<img>æ ‡ç­¾
	allATag := regexpFindAllImgTag.FindAllString(*htmlCode, -1)
	if len(allATag) == 0 {
		logs.Warn("find zero <img> tag from htmlCode")
		return nil
	}
	//ä»<img>æ ‡ç­¾ä¸­ç­›é€‰å‡ºé“¾æ¥ï¼Œè‹¥é…ç½®æœ‰æŒ‡å®šå…³é”®å­—åˆ™åªä»åŒ…å«å…³é”®å­—çš„æ ‡ç­¾ä¸­å–
	regexpFindLink := regexp.MustCompile(`src="[^"]*`)
	for i := 0; i < len(allATag); i++ {
		if linkKey != "" && !strings.Contains(allATag[i], linkKey) {
			continue
		}
		tmpLink := regexpFindLink.FindString(allATag[i])
		if len(tmpLink) < 7 {
			logs.Warn("find a danger url: %s", tmpLink)
			continue
		}
		tmpLink = tmpLink[5:] //å»é™¤src="å‰ç¼€
		//å°†ç›¸å¯¹é“¾æ¥è½¬æ¢æˆç»å¯¹é“¾æ¥
		if err := CheckUrlAndConver(baseUrl, &tmpLink); err != nil {
			logs.Warn("check url %s not pass, err=%v", tmpLink, err)
			continue
		}
		*link = append(*link, tmpLink)
	}
	return nil
}

//å¤„ç†æŒ‡å®šå·¥ä½œæ–¹å¼çš„é…ç½®å­—ç¬¦ä¸²ï¼Œå°†å…¶ä¸­çš„ä¿¡æ¯è§£æåˆ°å…¨å±€å˜é‡ä¹‹ä¸­
func setUpConfig(config string) error {
	sucNum, err := fmt.Sscanf(config, "%s %s %d %d %d %d %d %d %d %s %s %s %d %d",
		&method,
		&savePath,
		&totalSizeLimit,
		&numberLimit,
		&threadLimit,
		&minSizeLimit,
		&maxSizeLimit,
		&waitTimeLimit,
		&intervalTime,
		&baseUrl,
		&linkKey,
		&targetKey,
		&startPoint,
		&endPoint)
	if sucNum != 14 || err != nil {
		logs.Error("Scanf config from given string fail, err=", err)
		return errors.New("syntax worng")
	}
	if checkBaseConf() == false {
		err = errors.New("config checking not pass")
		logs.Error(err)
		return err
	}
	mainClient.Timeout = time.Second * time.Duration(waitTimeLimit)
	if len(pageUrlMap) > 0 {
		pageUrlMap = make(map[string]bool)
	}
	if len(imgUrlMap) > 0 {
		imgUrlMap = make(map[string]bool)
	}
	if foundPageList.Len() > 0 {
		foundPageList.Init()
	}
	if foundImgList.Len() > 0 {
		foundImgList.Init()
	}
	if method == "BFS" || method == "DFS" {
		lastPageEle = foundPageList.PushBack(baseUrl)
	}
	return nil
}

//æ£€æŸ¥ä¸é…ç½®ç›¸å…³çš„å…¨å±€å˜é‡ï¼Œè¿”å›æ£€æŸ¥ç»“æœ, ä¸€äº›ç‰¹å®šçš„éæ³•å€¼è¢«è®¾ç½®æˆé»˜è®¤å€¼è€ŒéæŠ¥é”™
func checkBaseConf() bool {
	if method != "BFS" && method != "DFS" && method != "FOR" && method != "LIST" {
		logs.Warn("config 'method' not right. method=%s", method)
		return false
	}
	if savePath == "" || checkDirExist(savePath) == false {
		logs.Warn("config 'savepath' not right. savePath=%s", savePath)
		return false
	}
	if totalSizeLimit <= 0 || totalSizeLimit > 100<<20 { //100GB
		logs.Warn("config 'totalSizeLimit' not right. totalSizeLimit=%d", totalSizeLimit)
		return false
	}
	if numberLimit <= 0 {
		logs.Warn("config 'numberLimit' erase from %d to 1", numberLimit)
		numberLimit = 1
	} else if numberLimit > 1000000 {
		logs.Warn("config 'numberLimit' erase from %d to 1000000", numberLimit)
		numberLimit = 1000000
	}
	if threadLimit <= 0 {
		logs.Warn("config 'threadLimit' erase from %d to 1", threadLimit)
		threadLimit = 1
	} else if threadLimit > 20 {
		logs.Warn("config 'threadLimit' erase from %d to 20", threadLimit)
		threadLimit = 20
	}
	if minSizeLimit < 0 {
		logs.Warn("config 'minSizeLimit' erase from %d to 0", minSizeLimit)
		minSizeLimit = 20
	}
	if maxSizeLimit < 0 || maxSizeLimit > 10240 { //100MB
		logs.Warn("config 'maxSizeLimit' erase from %d to 10240", maxSizeLimit)
		minSizeLimit = 20
	}
	if waitTimeLimit < 0 || waitTimeLimit > 10000 {
		logs.Warn("config 'waitTimeLimit' erase from %d to 0", waitTimeLimit)
		waitTimeLimit = 0
	}
	if intervalTime < 0 || intervalTime > 10000 {
		logs.Warn("config 'intervalTime' erase from %d to 0", intervalTime)
		intervalTime = 0
	}
	if isBaseUrlRight(baseUrl) == false {
		logs.Warn("config 'baseUrl' not right. baseUrl=%d", baseUrl)
		return false
	}
	if len(linkKey) > 50 {
		logs.Warn("config 'linkKey' is cancel because too long")
		linkKey = ""
	}
	if len(targetKey) > 50 {
		logs.Warn("config 'targetKey' is cancel because too long")
		targetKey = ""
	}
	if method == "FOR" && startPoint > endPoint {
		logs.Warn("config 'startPoint' and 'endPoing' swap value")
		tmp := startPoint
		startPoint = endPoint
		endPoint = tmp
	}
	return true
}

//è·å–ç”¨äºè¡¨ç¤ºå½“å‰å·¥ä½œçŠ¶æ€æŠ¥å‘Šä¿¡æ¯çš„å­—ç¬¦ä¸² ğŸ¢
func getReportString() (string, error) {
	var err error
	if diggerState == 0 { //æœªå¼€å§‹å·¥ä½œæˆ–å·²ç»ç»ˆæ­¢
		err = errors.New("Can't get report string because Digger is not working")
		return "", err
	}
	duration := time.Since(lastTime)
	lastTime = time.Now()
	speed := duration.Seconds()
	tmpBytes = 0
	percentage := 30
	reportString := fmt.Sprintf("%d %d %d %d %.2fKB/s %s %d", totalNumber, totalBytes, foundPageList.Len(),
		pageNumber, speed, time.Since(startTime), percentage)
	return reportString, nil
}

//æ£€æŸ¥æ˜¯å¦èƒ½æ­£å¸¸è®¿é—®baseURLæŒ‡å®šçš„ç¬¬ä¸€ä¸ªç½‘é¡µ,å¯æ£€æµ‹ç½‘ç»œçŠ¶æ€ä»¥åŠBaseUrl
func canVisitBaseUrl() (bool, error) {
	var err error
	if baseUrl == "" {
		err = errors.New("baseUrl is empty")
		logs.Error(err)
		return false, err
	}
	if mainClient == nil {
		err = errors.New("mainClient is null")
		logs.Error(err)
		return false, err
	}
	if resp, err := mainClient.Get(baseUrl); err != nil {
		logs.Warn("test fail: err=%v", err)
		return false, err
	} else if resp.StatusCode != http.StatusOK {
		err = fmt.Errorf("test fail: code=%d  status=%s", resp.StatusCode, resp.Status)
		logs.Warn(err)
		return false, err
	}
	return true, nil
}

//å†³å®šæŸä¸ªç½‘é¡µé“¾æ¥æ˜¯å¦åº”è¯¥è¿›å…¥ ğŸ¢
func isShouldDig(url string) bool {
	return false
}

//åˆ¤æ–­æŸä¸ªæ ‡ç­¾ä¸­æ˜¯å¦åŒ…å«ç›®æ ‡å›¾ç‰‡é“¾æ¥ ğŸ¢
func isHaveTargetImg(tag string) bool {
	return false
}

//åˆ¤æ–­æŸä¸ªæ ‡ç­¾ä¸­æ˜¯å¦åŒ…å«é…ç½®æŒ‡å®šçš„è½¬è·³é“¾æ¥ ğŸ¢
func isHaveSpecifHref(tag string) bool {
	return false
}

//åˆ¤æ–­baseurl æ ¼å¼æ˜¯å¦æ­£ç¡®å¯ç”¨
func isBaseUrlRight(baseurl string) bool {
	return true
}

//æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨  ğŸ“‚
func checkDirExist(dir string) bool {
	info, err := os.Stat(dir)
	if err != nil {
		if !os.IsNotExist(err) {
			logs.Error("error with file exist: %v", err)
		}
		return false
	}
	return info.IsDir()
}

//æ£€éªŒä¸€ä¸ªurlï¼Œä¸”å°†ç›¸å¯¹åœ°å€è½¬æ¢ä¸ºç»å¯¹åœ°å€,è‹¥æœ‰ä¸­æ–‡ç»è¿‡è½¬ç å°†ä¼šè¢«è¿˜åŸ ğŸ“‡
func CheckUrlAndConver(baseUrl string, targetUrl *string) error {
	if baseUrl == "" {
		return errors.New("baseUrl is empty")
	}
	if targetUrl == nil {
		return errors.New("targetUrl is nil")
	}
	target, err := url.Parse(*targetUrl)
	if err != nil {
		return err
	}
	//å°†ç»è¿‡è½¬ç çš„URLå­—ç¬¦ä¸²è¿˜åŸ
	if converLink, err := url.QueryUnescape(*targetUrl); err != nil {
		logs.Warn("QueryUnescape url %s fail: err=%d", *targetUrl, err)
	} else {
		*targetUrl = converLink
	}
	//è‹¥æœ¬èº«ä¸ºç»å¯¹è·¯å¾„åˆ™æ— éœ€è½¬æ¢
	if target.IsAbs() {
		return nil
	}
	base, err := url.Parse(baseUrl)
	if err != nil {
		return fmt.Errorf("BaseUrl not right. err=%v", err)
	}
	if !base.IsAbs() {
		return errors.New("BaseUrl is not absolute url")
	}
	*targetUrl = fmt.Sprintf("%s://%s%s", base.Scheme, base.Host, target.Path)
	return nil
}
